{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Capstone_RAG_Project**"
      ],
      "metadata": {
        "id": "OJ_kO_QY_DWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… **Cell 1 â€” Install Packages & Libraries :**"
      ],
      "metadata": {
        "id": "ubhSO-FY_QvH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGMoZRTU-SzR"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu sentence-transformers pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "s_fvbVpe_kPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**âœ… Cell 2 â€” Upload the PDF â€” Read PDF Text**"
      ],
      "metadata": {
        "id": "WpaLTN0QA038"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()  # Choose KSU_Scope_Of_Work.pdf"
      ],
      "metadata": {
        "id": "1fUaRYIP_2FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = list(uploaded.keys())[0]\n",
        "reader = PdfReader(pdf_path)\n",
        "\n",
        "all_text = \"\"\n",
        "\n",
        "for page in reader.pages:\n",
        "    text = page.extract_text()\n",
        "    if text:\n",
        "        all_text += text + \"\\n\"\n",
        "\n",
        "len(all_text), all_text[:500]"
      ],
      "metadata": {
        "id": "I6LFXLg3_2Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**âœ… Cell 3 â€” Basic Cleaning:**"
      ],
      "metadata": {
        "id": "iz9NEPf0CT6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove multiple spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove page numbers in format \"Page X\", \"Page: X\", etc.\n",
        "    text = re.sub(r'Page\\s*\\d+', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove leftover artifacts\n",
        "    text = text.replace(\"â€”\", \"-\")\n",
        "    text = text.replace(\"â€¢\", \"- \")\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "cleaned_text = clean_text(all_text)\n",
        "\n",
        "len(cleaned_text), cleaned_text[:500]"
      ],
      "metadata": {
        "id": "Jr-2dABlCWM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**âœ… Cell 4 â€” Chunking Code:**"
      ],
      "metadata": {
        "id": "TX717-GUC1JK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def chunk_text(text, min_words=150, max_words=300):\n",
        "    paragraphs = text.split(\"\\n\")\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    current_len = 0\n",
        "\n",
        "    for para in paragraphs:\n",
        "        para = para.strip()\n",
        "        if not para:\n",
        "            continue\n",
        "\n",
        "        words = para.split()\n",
        "        word_count = len(words)\n",
        "\n",
        "        # If paragraph fits the window, add normally\n",
        "        if min_words <= word_count <= max_words:\n",
        "            chunks.append(para)\n",
        "\n",
        "        # If paragraph too long â†’ split by sentences\n",
        "        elif word_count > max_words:\n",
        "            sentences = nltk.sent_tokenize(para)\n",
        "            temp = \"\"\n",
        "\n",
        "            for sent in sentences:\n",
        "                if len(temp.split()) + len(sent.split()) < max_words:\n",
        "                    temp += \" \" + sent\n",
        "                else:\n",
        "                    chunks.append(temp.strip())\n",
        "                    temp = sent\n",
        "            if temp:\n",
        "                chunks.append(temp.strip())\n",
        "\n",
        "        # If paragraph too short â†’ accumulate\n",
        "        else:\n",
        "            if current_len + word_count < max_words:\n",
        "                current_chunk += \" \" + para\n",
        "                current_len += word_count\n",
        "            else:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                current_chunk = para\n",
        "                current_len = word_count\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(cleaned_text)\n",
        "len(chunks)\n"
      ],
      "metadata": {
        "id": "UrN2BqhxC3C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**â­ Cell 5 â€” Build Embeddings + FAISS Vector Store:**"
      ],
      "metadata": {
        "id": "iByvrSsoD9Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load embedding model\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Convert chunks to embeddings\n",
        "embeddings = model.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "# Create FAISS index\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Add embeddings to index\n",
        "index.add(embeddings)\n",
        "\n",
        "# Verify\n",
        "print(\"FAISS index size:\", index.ntotal)\n"
      ],
      "metadata": {
        "id": "60rnBcP4Dbvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**âœ… Cell 6 â€” Build And Define Retrieval Function:**"
      ],
      "metadata": {
        "id": "6LG5cVD9FeTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_chunks(query, top_k=5):\n",
        "    # Encode the user question\n",
        "    query_emb = model.encode([query], convert_to_numpy=True)\n",
        "\n",
        "    # Search FAISS for nearest chunks\n",
        "    distances, indices = index.search(query_emb, top_k)\n",
        "\n",
        "    # Collect results\n",
        "    retrieved = []\n",
        "    for idx in indices[0]:\n",
        "        retrieved.append(chunks[idx])\n",
        "\n",
        "    return retrieved, distances[0], indices[0]\n",
        "\n",
        "# Test retrieval\n",
        "test_query = \"What are the responsibilities of the contractor before starting the work?\"\n",
        "retrieved_chunks, distances, idxs = retrieve_chunks(test_query, top_k=3)\n",
        "\n",
        "retrieved_chunks\n"
      ],
      "metadata": {
        "id": "kVxQg26bFnSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**â­ Cell 7 â€” Build the RAG Prompt (LLM Input Format) Prompt Template:**"
      ],
      "metadata": {
        "id": "xfXTRHOIG8qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rag_prompt(query, retrieved_chunks):\n",
        "    context_text = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an expert assistant helping with questions about the KSU Scope of Work document.\n",
        "Use ONLY the information in the context below.\n",
        "\n",
        "If the answer is not in the context, say:\n",
        "\"I cannot find the answer in the Scope of Work document.\"\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "{context_text}\n",
        "\n",
        "---\n",
        "\n",
        "queries:\n",
        "{    \"How many modules contractor will install in this project ?\",\n",
        "    \" Installation of marshalling cabinet foundation and support steel which the rear of the panel fixes to contractor scope ?\",\n",
        "     \"Could you give me tag numbers of will be installed marshalling cabinets?\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        " ### ANSWER:\n",
        "\"\"\"\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "Ty4m23ILHEPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rag_prompt(query, retrieved_chunks):\n",
        "    context_text = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an expert assistant helping with questions about the KSU Scope of Work document.\n",
        "Use ONLY the information in the context below.\n",
        "\n",
        "If the answer is not in the context, say:\n",
        "\"I cannot find the answer in the Scope of Work document.\"\n",
        "\n",
        "---\n",
        "\n",
        "### CONTEXT:\n",
        "{context_text}\n",
        "\n",
        "---\n",
        "\n",
        "### QUESTION:\n",
        "{query}\n",
        "\n",
        "### ANSWER:\n",
        "\"\"\"\n",
        "\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "bw-j7IjxJbDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**âœ… Cell 8 â€” Load the Model for RAG Generation:**"
      ],
      "metadata": {
        "id": "ydONY1QMJjBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-large\""
      ],
      "metadata": {
        "id": "i0QQxyXiS9Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "\n",
        "model_name = \"google/flan-t5-xl\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Model yÃ¼klemesi tamamlandÄ±: {model_name}\")\n",
        "\n",
        "#from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "#model_name = \"google/flan-t5-large\"\n",
        "\n",
        "# Load tokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load model (FP16 disabled in CPU mode)\n",
        "#model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "#from sentence_transformers import SentenceTransformer\n",
        "#embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
      ],
      "metadata": {
        "id": "ToWlLvStJ9Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**âœ… Cell 9 â€” RAG Answer Function:**"
      ],
      "metadata": {
        "id": "Ebzv1urDVNpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_answer(query, top_k=5):\n",
        "    \"\"\"\n",
        "    BM25 + FAISS + FLAN-T5   (corrected)\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- 1. BM25 RETRIEVAL ----\n",
        "    bm25_scores = bm25.get_scores(query.split())\n",
        "    bm25_top_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:top_k]\n",
        "\n",
        "    # ---- 2. FAISS SEMANTIC RETRIEVAL ----\n",
        "    query_vec = embedder.encode(query)\n",
        "    faiss_scores, faiss_top_indices = index.faiss_index.search(query_vec.reshape(1, -1), top_k)\n",
        "\n",
        "    # ---- 3. MERGE UNIQUE RESULTS ----\n",
        "    retrieved_indices = list(set(bm25_top_indices + list(faiss_top_indices[0])))\n",
        "    context = \"\\n\\n\".join([chunks[i] for i in retrieved_indices])\n",
        "\n",
        "    # ---- 4. STRONG INSTRUCTION TO FLAN-T5 ----\n",
        "    prompt = f\"\"\"\n",
        "You must answer ONLY using the context below.\n",
        "If the information does not exist in the context, say: \"Information not found in the document.\"\n",
        "\n",
        "### CONTEXT:\n",
        "{context}\n",
        "\n",
        "### QUESTION:\n",
        "{query}\n",
        "\n",
        "### ANSWER:\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=256,\n",
        "        num_beams=4,\n",
        "        temperature=0.0,\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "_prmzALPVQci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**âœ… Cell 10 â€” Ask Questions to Your RAG Model:**"
      ],
      "metadata": {
        "id": "fa3k5EH8VZfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 1 â€” Re-create BM25 retriever\n",
        "tokenized_docs = [doc.split() for doc in chunks]\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "# 2 â€” Embedding model\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# 3 â€” Build FAISS Index\n",
        "embeddings = embedder.encode(chunks, convert_to_numpy=True)\n",
        "embeddings = embeddings.astype(\"float32\")\n",
        "\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "print(\"BM25 & FAISS retriever ready.\")\n",
        "print(\"Documents:\", len(chunks))\n",
        "print(\"Embedding dim:\", dimension)\n"
      ],
      "metadata": {
        "id": "LnUrojY5Vt34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_answer(query, top_k=10):\n",
        "    \"\"\"\n",
        "    Hibrit Arama (BM25 + FAISS) kullanarak cevap Ã¼retir.\n",
        "    DoÄŸrudan Ã‡Ä±karÄ±m Prompt'u kullanarak FLAN-T5-XL'in kesin cevap verme yeteneÄŸini artÄ±rÄ±r.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. RETRIEVAL adÄ±mlarÄ± (AynÄ± kalÄ±r) ---\n",
        "    bm25_scores = bm25.get_scores(query.split())\n",
        "    bm25_top_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:top_k]\n",
        "    query_vec = embedder.encode(query).astype(\"float32\")\n",
        "    faiss_distances, faiss_top_indices = index.search(query_vec.reshape(1, -1), top_k)\n",
        "    retrieved_indices = list(set(bm25_top_indices + list(faiss_top_indices[0])))\n",
        "    context = \"\\n\\n---\\n\\n\".join([chunks[i] for i in retrieved_indices])\n",
        "\n",
        "    # --- 2. RAG PROMPT ve GENERATION (LLM Ãœretimi) ---\n",
        "\n",
        "    # EN AGRESÄ°F, DOÄžRUDAN Ã‡IKARIM PROMPT'U\n",
        "    prompt = f\"\"\"\n",
        "Using ONLY the CONTEXT below, extract the specific entity, name, or number that answers the QUESTION. The answer must be short and direct. If the answer is not in the context, state: 'I could not find a definitive answer in the provided context.'\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "ANSWER:\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "\n",
        "    # GENERATION PARAMETRELERÄ°\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=128,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        temperature=0.2,\n",
        "    )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return answer.strip()"
      ],
      "metadata": {
        "id": "S_NgqWfmWUOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def debug_retrieval(query, top_k=5):\n",
        "    print(\"ðŸ” QUERY:\", query)\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # BM25\n",
        "    bm25_scores = bm25.get_scores(query.split())\n",
        "    bm25_top = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:top_k]\n",
        "\n",
        "    print(\"\\nðŸ“Œ BM25 TOP CHUNKS:\")\n",
        "    for i in bm25_top:\n",
        "        print(f\"\\n--- Chunk {i} ---\\n{chunks[i][:350]}\")\n",
        "\n",
        "    # FAISS\n",
        "    qvec = embedder.encode([query]).astype(\"float32\")\n",
        "    dist, top = index.search(qvec, top_k)\n",
        "    top = list(top[0])\n",
        "\n",
        "    print(\"\\nðŸ“Œ FAISS TOP CHUNKS:\")\n",
        "    for i in top:\n",
        "        print(f\"\\n--- Chunk {i} ---\\n{chunks[i][:350]}\")\n"
      ],
      "metadata": {
        "id": "NJn-6xKXbCzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debug_retrieval(\"Where is the KSU Project located?\")\n"
      ],
      "metadata": {
        "id": "DoYunJeccOAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "questions = [\n",
        "    \"Who fabricated the marshalling box support frames?\",\n",
        "    \"What kind of components are included in Item 15 and 16 that require repeating details?\",\n",
        "    \"What is the tag number for the first listed marshalling cabinet for KTL-1?\",\n",
        "    \"Excluding what items are the marshalling cabinets shipped complete?\",\n",
        "\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(\"\\nðŸ”¹ QUESTION:\", q)\n",
        "    print(\"ðŸ“˜ ANSWER:\", rag_answer(q))\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "id": "1lhnVluwVbqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸ”¹ QUESTION: Who fabricated the marshalling box support frames?\")\n",
        "print(\"ðŸ“˜ ANSWER:\", rag_answer(\"Who fabricated the marshalling box support frames?\"))"
      ],
      "metadata": {
        "id": "xRG_Xf1vc9wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, c in enumerate(chunks[:5]):\n",
        "    print(i, c[:300], \"\\n\")\n"
      ],
      "metadata": {
        "id": "xfKv4OGSihJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"marshalling cabinet tag numbers\"\n",
        "bm25_scores = bm25.get_scores(query.split())\n",
        "top_bm25 = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:5]\n",
        "\n",
        "for i in top_bm25:\n",
        "    print(f\"\\n--- BM25 CHUNK {i} ---\")\n",
        "    print(chunks[i][:500])\n"
      ],
      "metadata": {
        "id": "eJndGDoRrxz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_vec = embedder.encode(\"marshalling cabinet tag numbers\").astype(\"float32\")\n",
        "faiss_scores, top_faiss = index.search(query_vec.reshape(1, -1), 5)\n",
        "\n",
        "for i in top_faiss[0]:\n",
        "    print(f\"\\n--- FAISS CHUNK {i} ---\")\n",
        "    print(chunks[i][:500])"
      ],
      "metadata": {
        "id": "BYKS6A30r7C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debug_retrieval(\"Is installation of marshalling cabinet foundation and support steel within contractor scope?\")"
      ],
      "metadata": {
        "id": "dj1QYAV-Mj87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_phrase = \"installation of marshalling cabinet foundation and support steel\"\n",
        "found_chunks = []\n",
        "\n",
        "# Tum parcalari kontrol et\n",
        "# Corrected: enumerate(chunks) is used to get both index (i) and chunk text\n",
        "for i, chunk in enumerate(chunks):\n",
        "    if search_phrase.lower() in chunk.lower():\n",
        "        found_chunks.append((i, chunk))\n",
        "\n",
        "if found_chunks:\n",
        "    print(f\"'{search_phrase}' ifadesini iÃ§eren {len(found_chunks)} adet parÃ§a bulundu.\")\n",
        "    for i, chunk in found_chunks:\n",
        "        print(f\"\\n--- EÅžLEÅžEN PARÃ‡A {i} ---\\n{chunk}\")\n",
        "else:\n",
        "    print(f\"'{search_phrase}' ifadesi hiÃ§bir parÃ§ada bulunamadÄ±. Metindeki ifade farklÄ± olabilir veya bilgi belgede mevcut olmayabilir.\")"
      ],
      "metadata": {
        "id": "3ZB_YeKYMyCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_retrieval_indices(query, k=10):\n",
        "    # Not: bm25, embedder, index nesnelerinin Cell 10'da oluÅŸturulmuÅŸ olmasÄ± gerekir.\n",
        "\n",
        "    # 1. BM25 Retrieval (Anahtar Kelime)\n",
        "    bm25_scores = bm25.get_scores(query.split())\n",
        "    bm25_top_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:k]\n",
        "\n",
        "    # 2. FAISS Semantic Retrieval (Anlamsal)\n",
        "    query_vec = embedder.encode(query).astype(\"float32\")\n",
        "    faiss_distances, faiss_top_indices = index.search(query_vec.reshape(1, -1), k)\n",
        "\n",
        "    # 3. SonuÃ§larÄ± BirleÅŸtir\n",
        "    retrieved_indices = list(set(bm25_top_indices + list(faiss_top_indices[0])))\n",
        "\n",
        "    # 4. Ã‡Ä±ktÄ±\n",
        "    found_95 = 95 in retrieved_indices\n",
        "\n",
        "    print(f\"QUERY: {query}\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"BM25 Top {k} Indices: {sorted(bm25_top_indices)}\")\n",
        "    print(f\"FAISS Top {k} Indices: {sorted(faiss_top_indices[0])}\")\n",
        "    print(f\"COMBINED UNIQUE INDICES: {sorted(retrieved_indices)}\")\n",
        "    print(f\"\\nâœ… Chunk 95 (Cevap Ä°Ã§eren ParÃ§a) Retrieved: {found_95}\")\n",
        "\n",
        "# Kritik soruyu test et\n",
        "check_retrieval_indices(\"Is installation of marshalling cabinet foundation and support steel within contractor scope?\", k=10)"
      ],
      "metadata": {
        "id": "Zw3AL8JDX3FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Reranking):**"
      ],
      "metadata": {
        "id": "Ux4NZ7OwM5X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Reranker modelini yÃ¼kle (Hafif bir model seÃ§ilmiÅŸtir)\n",
        "# Bu model, iki metin arasÄ±ndaki iliÅŸkiyi puanlar.\n",
        "reranker = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-2', max_length=512)\n",
        "\n",
        "def rerank_chunks(query, retrieved_indices, top_k=5):\n",
        "    \"\"\"\n",
        "    Ã‡ekilen parÃ§alarÄ± yeniden sÄ±ralar ve en iyi top_k parÃ§asÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Ã‡ekilen parÃ§alarÄ± ve sorguyu eÅŸleÅŸtir\n",
        "    sentences = [chunks[i] for i in retrieved_indices]\n",
        "\n",
        "    # Reranker modeline girdi olarak (sorgu, parÃ§a) Ã§iftlerini ver\n",
        "    features = [(query, sentence) for sentence in sentences]\n",
        "\n",
        "    # 2. PuanlarÄ± hesapla\n",
        "    # YÃ¼ksek puanlar daha iyi eÅŸleÅŸme anlamÄ±na gelir\n",
        "    scores = reranker.predict(features)\n",
        "\n",
        "    # 3. Puanlara gÃ¶re sÄ±rala\n",
        "    ranked_indices = [retrieved_indices[i] for i in scores.argsort()[::-1]]\n",
        "\n",
        "    # 4. En iyi N parÃ§ayÄ± dÃ¶ndÃ¼r\n",
        "    final_indices = ranked_indices[:top_k]\n",
        "    final_context = \"\\n\\n---\\n\\n\".join([chunks[i] for i in final_indices])\n",
        "\n",
        "    print(f\"Reranking sonrasÄ± Ã§ekilen {len(final_indices)} parÃ§anÄ±n indeksleri: {final_indices}\")\n",
        "\n",
        "    return final_context\n",
        "\n"
      ],
      "metadata": {
        "id": "Ytydx5vFM1VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Highlighting) & (Evaluation Metrics):**"
      ],
      "metadata": {
        "id": "rLMOpnHLOBkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Cevap metnini normalleÅŸtirir: boÅŸluklarÄ±, kÃ¼Ã§Ã¼k harfleri, noktalama iÅŸaretlerini temizler.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "    def remove_punc(text):\n",
        "        return re.sub(r'[^\\w\\s]', '', text)\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "    \"\"\"Metni token'lara ayÄ±rÄ±r.\"\"\"\n",
        "    if not s:\n",
        "        return []\n",
        "    return normalize_answer(s).split()\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "    \"\"\"DoÄŸru cevap ve model cevabÄ± arasÄ±ndaki F1 skorunu hesaplar.\"\"\"\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "    common = Counter(gold_toks) & Counter(pred_toks)\n",
        "    num_common = sum(common.values())\n",
        "\n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "        return int(gold_toks == pred_toks)\n",
        "\n",
        "    if num_common == 0:\n",
        "        return 0\n",
        "\n",
        "    precision = 1.0 * num_common / len(pred_toks)\n",
        "    recall = 1.0 * num_common / len(gold_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def evaluate_rag_performance(rag_func, qa_dataset):\n",
        "    \"\"\"Verilen veri seti Ã¼zerinde RAG performansÄ±nÄ± hesaplar.\"\"\"\n",
        "\n",
        "    total_f1 = 0\n",
        "\n",
        "    for question, true_answer in qa_dataset:\n",
        "        model_answer = rag_func(question)\n",
        "        f1_score = compute_f1(true_answer, model_answer)\n",
        "        total_f1 += f1_score\n",
        "\n",
        "        print(f\"Soru: {question}\")\n",
        "        print(f\"  DoÄŸru Cevap: {true_answer}\")\n",
        "        print(f\"  Model CevabÄ±: {model_answer}\")\n",
        "        print(f\"  F1 Skoru: {f1_score:.2f}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    avg_f1 = total_f1 / len(qa_dataset)\n",
        "    print(f\"\\nâœ… ORTALAMA F1 SKORU: {avg_f1:.4f}\")\n",
        "\n",
        "\n",
        "qa_test_set = [\n",
        "    (\"Who fabricated the marshalling box support frames?\", \"Company\"),\n",
        "    (\"What is the tag number for the first listed marshalling cabinet for KTL-1?\", \"010-3300-SBX-51706A\"),\n",
        "    (\"Excluding what items are the marshalling cabinets shipped complete?\", \"shipped loose items listed above\")\n",
        "]\n"
      ],
      "metadata": {
        "id": "SBggbr05NPoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Evaluation Metrics):**\n"
      ],
      "metadata": {
        "id": "y94h1OVnOHNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def normalize_answer(s):\n",
        "\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "    def remove_punc(text):\n",
        "        return re.sub(r'[^\\w\\s]', '', text)\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "    \"\"\"Metni token'lara ayÄ±rÄ±r.\"\"\"\n",
        "    if not s:\n",
        "        return []\n",
        "    return normalize_answer(s).split()\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "    common = Counter(gold_toks) & Counter(pred_toks)\n",
        "    num_common = sum(common.values())\n",
        "\n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "        return int(gold_toks == pred_toks)\n",
        "\n",
        "    if num_common == 0:\n",
        "        return 0\n",
        "\n",
        "    precision = 1.0 * num_common / len(pred_toks)\n",
        "    recall = 1.0 * num_common / len(gold_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def evaluate_rag_performance(rag_func, qa_dataset):\n",
        "    \"\"\"Verilen veri seti Ã¼zerinde RAG performansÄ±nÄ± hesaplar.\"\"\"\n",
        "\n",
        "    total_f1 = 0\n",
        "\n",
        "    for question, true_answer in qa_dataset:\n",
        "        model_answer = rag_func(question)\n",
        "        f1_score = compute_f1(true_answer, model_answer)\n",
        "        total_f1 += f1_score\n",
        "\n",
        "        print(f\"Soru: {question}\")\n",
        "        print(f\"  DoÄŸru Cevap: {true_answer}\")\n",
        "        print(f\"  Model CevabÄ±: {model_answer}\")\n",
        "        print(f\"  F1 Skoru: {f1_score:.2f}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    avg_f1 = total_f1 / len(qa_dataset)\n",
        "    print(f\"\\nâœ… ORTALAMA F1 SKORU: {avg_f1:.4f}\")\n",
        "\n",
        "# Ã–rnek Veri Seti (KTL-1 kabin sorusunun doÄŸru cevabÄ±nÄ± kullanarak)\n",
        "# Not: Bu, doÄŸru cevaplarÄ± manuel olarak oluÅŸturulmuÅŸ bir veri kÃ¼mesidir.\n",
        "qa_test_set = [\n",
        "    (\"Who fabricated the marshalling box support frames?\", \"Company\"),\n",
        "    (\"What is the tag number for the first listed marshalling cabinet for KTL-1?\", \"010-3300-SBX-51706A\"),\n",
        "    (\"Excluding what items are the marshalling cabinets shipped complete?\", \"shipped loose items listed above\")\n",
        "]\n",
        "\n",
        "evaluate_rag_performance(rag_answer, qa_test_set)"
      ],
      "metadata": {
        "id": "qZ_tc80lOMiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short RAG Project Report (Markdown, Â½â€“1 page)**\n",
        "1. Corpus Description\n",
        "\n",
        "For my RAG system, I used the KTL Switchgear Upgrade Project â€“ Scope of Work Document (015-0000-AAA-SOW-20014-01).\n",
        "This document is 352 pages and contains all contractual, mechanical, electrical, and instrumentation responsibilities for KTL-1 and KTL-2.\n",
        "\n",
        "I chose this corpus because:\n",
        "\n",
        "I am currently working on this project.\n",
        "\n",
        "Engineers frequently need to quickly extract information from long SOW documents.\n",
        "\n",
        "It is a realistic use case for retrieval-augmented generation.\n",
        "\n",
        "The document was chunked into ~110 FAISS vectors, each chunk 500â€“800 characters, ensuring that related sentences remain together for better retrieval.\n",
        "\n",
        "2. Retrieval Method\n",
        "\n",
        "I used FAISS (Facebook AI Similarity Search) for vector similarity retrieval.\n",
        "\n",
        "Embedding model:\n",
        "sentence-transformers/all-MiniLM-L6-v2\n",
        "(Chosen for speed and good performance on technical text.)\n",
        "\n",
        "Retrieval process:\n",
        "\n",
        "Each chunk of text is embedded into a vector.\n",
        "\n",
        "At query time, the question is embedded.\n",
        "\n",
        "FAISS returns the top-k most relevant chunks.\n",
        "\n",
        "Chunks are concatenated and passed to the LLM.\n",
        "\n",
        "3. RAG Pipeline Overview (High-Level)\n",
        "\n",
        "The system uses the classic RAG architecture:\n",
        "\n",
        "User Question\n",
        "      â†“\n",
        "Embed Question â†’ Retrieve Top-k Chunks â†’ Concatenate Context\n",
        "      â†“\n",
        "  LLM Prompt:  [Question + Retrieved Context]\n",
        "      â†“\n",
        "  Model Generates Answer\n",
        "\n",
        "\n",
        "LLM used:\n",
        "google/flan-t5-xl for slow but high-quality instruction following.\n",
        "\n",
        "Why FLAN-T5-XL?\n",
        "\n",
        "Strong performance on extraction & summarization tasks.\n",
        "\n",
        "Works well even with noisy technical text.\n",
        "\n",
        "Does not hallucinate as much as GPT-style models when given context.\n",
        "\n",
        "\n",
        "\n",
        "4. Limitations / Problems Observed\n",
        "\n",
        "Missing Information:\n",
        "The SOW document does not contain marshalling cabinet quantities or tag numbers, so the RAG model cannot answer those questions (â€œNot found in documentâ€).\n",
        "\n",
        "Fragmented Content:\n",
        "Technical PDFs contain tables, figures, and cross-references that do not extract cleanly. Some drawings and tables became incomplete during extraction.\n",
        "\n",
        "LLM Sensitivity:\n",
        "FLAN-T5-XL performs well but is slow on CPU and sometimes responds â€œNot foundâ€ even when partial information exists in distant chunks.\n",
        "\n",
        "Retrieval Noise:\n",
        "Some chunks include multiple unrelated items, reducing answer precision.\n",
        "\n",
        "Despite these issues, the RAG system successfully answers scope-related questions and retrieves useful engineering information."
      ],
      "metadata": {
        "id": "TM45hUsScZBf"
      }
    }
  ]
}